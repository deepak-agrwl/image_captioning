{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Download the Flickr8k dataset from Kaggle",
   "id": "859e3d8f90570d31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T04:47:30.242626Z",
     "start_time": "2025-05-25T04:47:26.244166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install kaggle\n",
    "!pip install kagglehub"
   ],
   "id": "a006dcca7e06a1b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\r\n",
      "  Obtaining dependency information for kaggle from https://files.pythonhosted.org/packages/14/83/7f29c7abe0d5dc769dad7da993382c3e4239ad63e1dd58414d129e0a4da2/kaggle-1.7.4.5-py3-none-any.whl.metadata\r\n",
      "  Downloading kaggle-1.7.4.5-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: bleach in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (6.2.0)\r\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (2025.1.31)\r\n",
      "Requirement already satisfied: charset-normalizer in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (3.4.1)\r\n",
      "Requirement already satisfied: idna in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (3.10)\r\n",
      "Requirement already satisfied: protobuf in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (5.29.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (2.9.0.post0)\r\n",
      "Collecting python-slugify (from kaggle)\r\n",
      "  Obtaining dependency information for python-slugify from https://files.pythonhosted.org/packages/a4/62/02da182e544a51a5c3ccf4b03ab79df279f9c60c5e82d5e8bec7ca26ac11/python_slugify-8.0.4-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\r\n",
      "Requirement already satisfied: requests in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (75.8.0)\r\n",
      "Requirement already satisfied: six>=1.10 in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (1.17.0)\r\n",
      "Collecting text-unidecode (from kaggle)\r\n",
      "  Obtaining dependency information for text-unidecode from https://files.pythonhosted.org/packages/a6/a5/c0b6468d3824fe3fde30dbb5e1f687b291608f9473681bbf7dabbf5a87d7/text_unidecode-1.3-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Requirement already satisfied: tqdm in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (4.67.1)\r\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (2.3.0)\r\n",
      "Requirement already satisfied: webencodings in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kaggle) (0.5.1)\r\n",
      "Downloading kaggle-1.7.4.5-py3-none-any.whl (181 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m181.2/181.2 kB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\r\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.2/78.2 kB\u001B[0m \u001B[31m10.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: text-unidecode, python-slugify, kaggle\r\n",
      "Successfully installed kaggle-1.7.4.5 python-slugify-8.0.4 text-unidecode-1.3\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting kagglehub\r\n",
      "  Obtaining dependency information for kagglehub from https://files.pythonhosted.org/packages/49/bf/c2a24567bb6bd80c1fe7cb2ed1a332666476f69c313256aff96094bef93e/kagglehub-0.3.12-py3-none-any.whl.metadata\r\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\r\n",
      "Requirement already satisfied: packaging in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kagglehub) (24.2)\r\n",
      "Requirement already satisfied: pyyaml in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kagglehub) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kagglehub) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from kagglehub) (4.67.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from requests->kagglehub) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from requests->kagglehub) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from requests->kagglehub) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/deagrawa/PycharmProjects/AIML-DeepLearning/.venv/lib/python3.13/site-packages (from requests->kagglehub) (2025.1.31)\r\n",
      "Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m68.0/68.0 kB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: kagglehub\r\n",
      "Successfully installed kagglehub-0.3.12\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T05:37:12.578996Z",
     "start_time": "2025-05-25T05:26:17.910571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "# Authenticate Kaggle API and download the Flickr8k dataset\n",
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Define the dataset and download path\n",
    "dataset_flickr8k = \"adityajn105/flickr8k\"\n",
    "download_path_flickr8k = \"./flickr8k\"\n",
    "\n",
    "# Download and extract the dataset\n",
    "if not os.path.exists(download_path_flickr8k):\n",
    "    os.makedirs(download_path_flickr8k)\n",
    "\n",
    "api.dataset_download_files(dataset_flickr8k, path=download_path_flickr8k, unzip=True)\n",
    "\n",
    "print(\"Flickr8k dataset downloaded and extracted to:\", download_path_flickr8k)\n",
    "\n",
    "# Download the Flickr30K Image Dataset from Kaggle\n",
    "# Define the dataset and download path\n",
    "dataset_flickr30k = \"hsankesara/flickr-image-dataset\"\n",
    "download_path_flickr30k = \"./flickr30k\"\n",
    "\n",
    "# Download and extract the dataset\n",
    "if not os.path.exists(download_path_flickr30k):\n",
    "    os.makedirs(download_path_flickr30k)\n",
    "\n",
    "api.dataset_download_files(dataset_flickr30k, path=download_path_flickr30k, unzip=True)\n",
    "\n",
    "print(\"Flickr30K dataset downloaded and extracted to:\", download_path_flickr30k)"
   ],
   "id": "d9e646b488c34825",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/deagrawa/.kaggle/kaggle.json'\n",
      "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
      "Flickr8k dataset downloaded and extracted to: ./flickr8k\n",
      "Dataset URL: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
      "Flickr30K dataset downloaded and extracted to: ./flickr30k\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate embeddings for images in the Flickr8k dataset using a pretrained CLIP model",
   "id": "c220afb2bad188c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:00:05.011102Z",
     "start_time": "2025-05-25T05:59:23.610961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load model and processor once\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def get_image_clip_embedding(image_path):\n",
    "    \"\"\"Returns the CLIP embedding for an image.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "    return outputs.squeeze().numpy()\n",
    "\n",
    "def get_text_clip_embedding(text):\n",
    "    \"\"\"Returns the CLIP embedding for a text string.\"\"\"\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_text_features(**inputs)\n",
    "    return outputs.squeeze().numpy()\n"
   ],
   "id": "5db355a6cfa69834",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T06:32:55.583851Z",
     "start_time": "2025-05-25T06:12:23.087520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# how to load the kaggle fickr dataset and use the above methods to generate embeddings for each image and text\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "flickr8k_images_path = \"./flickr8k/Images\"\n",
    "flickr8k_captions_path = \"./flickr8k/captions.txt\"\n",
    "\n",
    "# Load captions\n",
    "def load_captions(captions_file):\n",
    "    \"\"\"Loads captions from the Flickr8k captions file.\"\"\"\n",
    "    captions = {}\n",
    "    with open(captions_file, \"r\") as file:\n",
    "        next(file)  # Skip the header\n",
    "        for line in file:\n",
    "            image_name, caption = line.strip().split(\",\",1)\n",
    "            #image_name = image_name.split(\"#\")[0]  # Remove the #index\n",
    "            if image_name not in captions:\n",
    "                captions[image_name] = []\n",
    "            captions[image_name].append(caption)\n",
    "    return captions\n",
    "\n",
    "# Generate embeddings\n",
    "def generate_embeddings(images_path, captions, output_path=\"./embeddings.json\"):\n",
    "    \"\"\"Generates and saves embeddings for images and captions.\"\"\"\n",
    "    embeddings = {}\n",
    "    for image_name, captions_list in tqdm(captions.items(), desc=\"Processing images\"):\n",
    "        image_path = os.path.join(images_path, image_name)\n",
    "        if os.path.exists(image_path):\n",
    "            # Generate image embedding\n",
    "            image_embedding = get_image_clip_embedding(image_path)\n",
    "            # Generate text embeddings for all captions\n",
    "            text_embeddings = [get_text_clip_embedding(caption) for caption in captions_list]\n",
    "            # Store embeddings\n",
    "            embeddings[image_name] = {\n",
    "                \"image_embedding\": image_embedding.tolist(),\n",
    "                \"text_embeddings\": [embedding.tolist() for embedding in text_embeddings],\n",
    "            }\n",
    "    # Save embeddings to a JSON file\n",
    "    with open(output_path, \"w\") as file:\n",
    "        json.dump(embeddings, file)\n",
    "    print(f\"Embeddings saved to {output_path}\")\n",
    "\n",
    "# Load captions\n",
    "captions = load_captions(flickr8k_captions_path)\n",
    "\n",
    "# Generate and save embeddings\n",
    "generate_embeddings(flickr8k_images_path, captions)"
   ],
   "id": "e33e061f1f9069eb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 8091/8091 [16:31<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to ./embeddings.json\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:49:12.161191Z",
     "start_time": "2025-05-31T08:49:12.159199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the generated embeddings for text_embeddings from the JSON file in memory\n",
    "# And take an embedding of an image and then find the most similar text embedding  based on cosine similarity, dumping the cosine similarity value per text embedding in a file\n"
   ],
   "id": "6f9d40029d180b17",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bee6a71324ba8fe3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
