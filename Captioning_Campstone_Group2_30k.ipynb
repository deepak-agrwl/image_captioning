{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e423616b",
   "metadata": {},
   "source": [
    "Download the Flickr30K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a0579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset already exists. Skipping download. Located at: flickr30k_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devashri/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Set output folder\n",
    "output_dir = 'flickr30k_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Dataset identifier from Kaggle URL\n",
    "dataset = 'hsankesara/flickr-image-dataset'\n",
    "\n",
    "# Check for a file that should exist after extraction\n",
    "expected_file = os.path.join(output_dir, 'results.csv')  # Change if another file is more reliable\n",
    "\n",
    "if not os.path.exists(expected_file):\n",
    "    print(\"Dataset not found locally. Downloading from Kaggle...\")\n",
    "    api.dataset_download_files(dataset, path=output_dir, unzip=True)\n",
    "    print(f\"✅ Download complete! Files saved to: {output_dir}\")\n",
    "else:\n",
    "    print(f\"✅ Dataset already exists. Skipping download. Located at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e5542",
   "metadata": {},
   "source": [
    "Install dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71ed1695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/devashri/Library/Python/3.9/lib/python/site-packages (2.3.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/devashri/Library/Python/3.9/lib/python/site-packages (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: filelock in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/devashri/Library/Python/3.9/lib/python/site-packages (4.52.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: filelock in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: requests in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: albumentations in /Users/devashri/Library/Python/3.9/lib/python/site-packages (2.0.8)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albumentations) (2.11.7)\n",
      "Requirement already satisfied: PyYAML in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albumentations) (1.13.1)\n",
      "Requirement already satisfied: albucore==0.0.24 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albumentations) (2.0.2)\n",
      "Requirement already satisfied: eval-type-backport in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albumentations) (0.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albumentations) (4.12.2)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from albucore==0.0.24->albumentations) (6.4.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from timm) (2.7.1)\n",
      "Requirement already satisfied: safetensors in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from timm) (0.5.3)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp39-cp39-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 85.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from timm) (0.33.0)\n",
      "Requirement already satisfied: pyyaml in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface_hub->timm) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface_hub->timm) (1.1.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface_hub->timm) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: filelock in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from huggingface_hub->timm) (3.18.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from requests->huggingface_hub->timm) (2025.6.15)\n",
      "Requirement already satisfied: jinja2 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading pillow-11.2.1-cp39-cp39-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 16.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/devashri/Library/Python/3.9/lib/python/site-packages (from torchvision->timm) (2.0.2)\n",
      "Installing collected packages: pillow, torchvision, timm\n",
      "Successfully installed pillow-11.2.1 timm-1.0.15 torchvision-0.22.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install albumentations\n",
    "%pip install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc6b3f",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eb6ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption_number</th>\n",
       "      <th>caption</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image caption_number  \\\n",
       "0  1000092795.jpg              0   \n",
       "1  1000092795.jpg              1   \n",
       "2  1000092795.jpg              2   \n",
       "3  1000092795.jpg              3   \n",
       "4  1000092795.jpg              4   \n",
       "\n",
       "                                             caption  id  \n",
       "0  Two young guys with shaggy hair look at their ...   0  \n",
       "1  Two young , White males are outside near many ...   0  \n",
       "2   Two men in green shirts are standing in a yard .   0  \n",
       "3       A man in a blue shirt standing in a garden .   0  \n",
       "4            Two friends enjoy time spent together .   0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join(output_dir, 'results.csv'), delimiter=\"|\")\n",
    "df.columns = ['image', 'caption_number', 'caption']\n",
    "df['caption'] = df['caption'].str.strip()\n",
    "df['caption_number'] = df['caption_number'].str.strip()\n",
    "df.loc[19999, 'caption_number'] = \"4\"\n",
    "df.loc[19999, 'caption'] = \"A dog runs across the grass .\"\n",
    "ids = [id_ for id_ in range(len(df) // 5) for i in range(5)]\n",
    "df['id'] = ids\n",
    "df.to_csv(\"captions.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2a9e0",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f43197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = os.path.join(output_dir, 'flickr30k_images/flickr30k_images')\n",
    "\n",
    "    # Set to the current directory\n",
    "    captions_path = \".\"\n",
    "\n",
    "    # Number of samples processed per batch during training.\n",
    "    batch_size = 64\n",
    "\n",
    "    # Number of subprocesses used for data loading\n",
    "    num_workers = 4\n",
    "\n",
    "    # Learning rates\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "\n",
    "    # Training strategy\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 2\n",
    "\n",
    "    # Device selection\n",
    "    device = \"mps\" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Model configuration\n",
    "    model_name = 'resnet50'\n",
    "    image_embedding = 2048\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a9a16",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f5b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda7f9b",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b03ab4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=cfg.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = cv2.imread(f\"{cfg.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg.size, cfg.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(cfg.size, cfg.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8116348",
   "metadata": {},
   "source": [
    "Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d34decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=cfg.model_name, pretrained=cfg.pretrained, trainable=cfg.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a2533",
   "metadata": {},
   "source": [
    "Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0569bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=cfg.text_encoder_model, pretrained=cfg.pretrained, trainable=cfg.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c3bb4",
   "metadata": {},
   "source": [
    "Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4632368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=cfg.projection_dim,\n",
    "        dropout=cfg.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03750cc",
   "metadata": {},
   "source": [
    "CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa0d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=cfg.temperature,\n",
    "        image_embedding=cfg.image_embedding,\n",
    "        text_embedding=cfg.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5619104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# A simple Example\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 4\n",
    "dim = 256\n",
    "embeddings = torch.randn(batch_size, dim)\n",
    "out = embeddings @ embeddings.T\n",
    "print(F.softmax(out, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523cf709",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece7d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(f\"{cfg.captions_path}/captions.csv\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not cfg.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe\n",
    "\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=cfg.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "892e7712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(cfg.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(cfg.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ce05e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mr/_xqy4pmj3zb2w_zvq3m0gnjw0000gn/T/ipykernel_11069/870235430.py:40: UserWarning: Argument(s) 'always_apply' are not valid for transform Resize\n",
      "  A.Resize(cfg.size, cfg.size, always_apply=True),\n",
      "/var/folders/mr/_xqy4pmj3zb2w_zvq3m0gnjw0000gn/T/ipykernel_11069/870235430.py:41: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
      "/var/folders/mr/_xqy4pmj3zb2w_zvq3m0gnjw0000gn/T/ipykernel_11069/870235430.py:47: UserWarning: Argument(s) 'always_apply' are not valid for transform Resize\n",
      "  A.Resize(cfg.size, cfg.size, always_apply=True),\n",
      "/var/folders/mr/_xqy4pmj3zb2w_zvq3m0gnjw0000gn/T/ipykernel_11069/870235430.py:48: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(max_pixel_value=255.0, always_apply=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1987 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'CLIPDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import albumentations as A\n",
    "import timm\n",
    "\n",
    "train_df, valid_df = make_train_valid_dfs()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "model = CLIPModel().to(CFG.device)\n",
    "\n",
    "params = [\n",
    "    {\"params\": model.image_encoder.parameters(), \"lr\": cfg.image_encoder_lr},\n",
    "    {\"params\": model.text_encoder.parameters(), \"lr\": cfg.text_encoder_lr},\n",
    "    {\"params\": itertools.chain(\n",
    "        model.image_projection.parameters(), model.text_projection.parameters()\n",
    "    ), \"lr\": cfg.head_lr, \"weight_decay\": cfg.weight_decay}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=cfg.patience, factor=cfg.factor\n",
    ")\n",
    "step = \"epoch\"\n",
    "\n",
    "best_loss = float('inf')\n",
    "for epoch in range(cfg.epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    model.train()\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = valid_epoch(model, valid_loader)\n",
    "\n",
    "    if valid_loss.avg < best_loss:\n",
    "        best_loss = valid_loss.avg\n",
    "        torch.save(model.state_dict(), \"best.pt\")\n",
    "        print(\"Saved Best Model!\")\n",
    "\n",
    "    lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c365b",
   "metadata": {},
   "source": [
    "Getting Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(valid_df, model_path):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    \n",
    "    model = CLIPModel().to(cfg.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=cfg.device))\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(cfg.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449627c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, valid_df = make_train_valid_dfs()\n",
    "model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(cfg.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "    \n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "    \n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "    \n",
    "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image = cv2.imread(f\"{cfg.image_path}/{match}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534fe922",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_matches(model, \n",
    "             image_embeddings,\n",
    "             query=\"one dog sitting on the grass\",\n",
    "             image_filenames=valid_df['image'].values,\n",
    "             n=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
